{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlphaZero_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPGeTEpTJVKW3j7NcEll2pb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/AlphaGo/blob/main/AlphaZero_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_DJerhtkgAU"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from math import sqrt\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler, LambdaCallback\n",
        "from tensorflow.keras.layers import Activation, Add, BatchNormalization, Conv2D, Dense, GlobalAvgPool2D, Input\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.utils import plot_model, Progbar\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "import random, string"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbNN_S8hUaQP"
      },
      "source": [
        "class State:\n",
        "    def __init__(self, pieces=None, enemy_pieces=None):\n",
        "        self.pieces, self.enemy_pieces = np.zeros(9), np.zeros(9)\n",
        "        if pieces is not None:\n",
        "            self.pieces = pieces\n",
        "        if enemy_pieces is not None:\n",
        "            self.enemy_pieces = enemy_pieces\n",
        "\n",
        "    def step(self, action):\n",
        "        pieces_cp = np.copy(self.pieces)\n",
        "        pieces_cp[action] = 1\n",
        "        return State(self.enemy_pieces, pieces_cp)\n",
        "\n",
        "    def judgment_lose(self):\n",
        "        val = np.where(self.enemy_pieces == 1)[0]\n",
        "        arr_corect = np.array([[0,1,2],[3,4,5],[6,7,8],[0,3,6],\n",
        "                               [1,4,7],[2,5,8],[0,4,8],[2,4,6]])\n",
        "        for arr in arr_corect:    \n",
        "            if set(arr) <= set(val):\n",
        "                return True\n",
        "        return False\n",
        "    \n",
        "    def terminal(self):\n",
        "        standoff = (len(np.where(self.pieces == 1)[0]) + len(np.where(self.enemy_pieces == 1)[0]) == 9)\n",
        "        return self.judgment_lose() or standoff\n",
        "\n",
        "    def possible_actions(self):\n",
        "        actions = self.pieces + self.enemy_pieces\n",
        "        actions = np.where(actions == 0)[0]\n",
        "        return actions\n",
        "\n",
        "    def first_attack(self):\n",
        "        return (len(np.where(self.pieces == 0)[0]) == len(np.where(self.enemy_pieces == 0)[0]))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYpIu2w2lx1c"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self, loadmodel = False):\n",
        "        self.filters = 128\n",
        "        self.hidden_layers = 16\n",
        "        self.obs_shape = (3, 3, 2)\n",
        "        self.nn_actions = 9\n",
        "        self.kr = l2(0.0005)\n",
        "        self.opt = SGD(learning_rate = 0.001, momentum = 0.9)\n",
        "\n",
        "        if not loadmodel:\n",
        "            self._main_network_layer()\n",
        "        else:\n",
        "            self._load()\n",
        "\n",
        "    def _main_network_layer(self):\n",
        "        input = Input(shape = self.obs_shape)\n",
        "        x = self._conv_layer(self.filters)(input)\n",
        "        for i in range(self.hidden_layers):\n",
        "            x = self._residual_layer()(x)\n",
        "        x = GlobalAvgPool2D()(x)\n",
        "\n",
        "        p = Dense(self.nn_actions, kernel_regularizer=self.kr, activation='softmax')(x)\n",
        "        v = Dense(1, kernel_regularizer=self.kr, activation='tanh')(x)\n",
        "\n",
        "        model = Model(inputs = input, outputs=[p, v])\n",
        "        model.compile(loss = ['categorical_crossentropy', 'mse'], optimizer = self.opt)\n",
        "        self.model = model\n",
        "\n",
        "        dot_img_file = './alphazero_model.png'\n",
        "        plot_model(self.model, to_file=dot_img_file, show_shapes=True)\n",
        "\n",
        "    def _residual_layer(self):\n",
        "        def f(input_block):\n",
        "            x = self._conv_layer(self.filters)(input_block)\n",
        "            x = self._conv_layer(self.filters, join_act = False)(x)\n",
        "            x = Add()([x, input_block])\n",
        "            x = Activation('relu')(x)\n",
        "            return x\n",
        "        return f\n",
        "\n",
        "    def _conv_layer(self, filters, join_act = True):\n",
        "        def f(input_block):\n",
        "            x = Conv2D(filters, 3, padding='same', use_bias=False,\n",
        "                       kernel_initializer='he_normal',\n",
        "                       kernel_regularizer=self.kr)(input_block)\n",
        "            x = BatchNormalization()(x)\n",
        "            if join_act:\n",
        "                x = Activation('relu')(x)\n",
        "            return x\n",
        "        return f \n",
        "        \n",
        "    def predict(self, state):\n",
        "        a, b, c = self.obs_shape\n",
        "        cc = np.concatenate([state.pieces, state.enemy_pieces])\n",
        "        cc = np.reshape(cc, [1, 2, 9])\n",
        "        cc = cc.reshape(c, a, b).transpose(1, 2, 0).reshape(1, a, b, c)\n",
        "\n",
        "        y = self.model.predict(cc, batch_size = 1)\n",
        "\n",
        "        policies = y[0][0][state.possible_actions()]\n",
        "        policies /= sum(policies) if sum(policies) else 1\n",
        "\n",
        "        value = y[1][0][0]\n",
        "        return policies, value\n",
        "\n",
        "    def save(self):\n",
        "        self.model.save('./alphazero.h5')\n",
        "\n",
        "    def _load(self):\n",
        "        self.model = load_model('./alphazero.h5')\n",
        "\n",
        "    def train(self, trajectory):\n",
        "        piece, policy, value = trajectory.get_trajectory()\n",
        "        a, b, c = self.obs_shape\n",
        "        piece = piece.reshape(len(piece), c, a, b).transpose(0, 2, 3, 1)\n",
        "        self.model.fit(piece, [policy, value], batch_size=128, epochs = 100, verbose=0)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMG52yEp5WNY"
      },
      "source": [
        "class MonteCarloTreeSearch:\n",
        "    def __init__(self, brain):\n",
        "        self.brain = brain\n",
        "        self.search_num = 50\n",
        "        self.temp = 1.0\n",
        "\n",
        "    def search(self, state):\n",
        "        node = Node(state, 0, self.brain)\n",
        "        for _ in range(self.search_num): node.evaluate()\n",
        "\n",
        "        scores = np.array([i.n for i in node.child_nodes])\n",
        "        scores = self._boltzman_distribution(scores)\n",
        "        return scores\n",
        "\n",
        "    def _boltzman_distribution(self, ps):\n",
        "        ps = ps ** (1 / self.temp)\n",
        "        return ps / np.sum(ps)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdQtM6hxYFKA"
      },
      "source": [
        "class Node:\n",
        "    def __init__(self, state, p, brain):\n",
        "        self.state = state\n",
        "        self.p = p\n",
        "        self.w = 0\n",
        "        self.n = 0\n",
        "        self.c_puct = 1.0\n",
        "        self.obs_shape = (3, 3, 2)\n",
        "        self.child_nodes = None\n",
        "        self.brain = brain\n",
        "\n",
        "    def evaluate(self):\n",
        "        if self.state.terminal():\n",
        "            value = -1 if self.state.judgment_lose() else 0\n",
        "            self.w += value\n",
        "            self.n += 1\n",
        "            return value\n",
        "\n",
        "        if self.child_nodes is None:\n",
        "            policies, value = self.brain.predict(self.state)\n",
        "            self.w += value\n",
        "            self.n += 1\n",
        "\n",
        "            self.child_nodes = np.array([])\n",
        "            for action, policy in zip(self.state.possible_actions(), policies):\n",
        "                node = Node(self.state.step(action), policy, self.brain)\n",
        "                self.child_nodes = np.append(self.child_nodes, node)\n",
        "            return value\n",
        "        else:\n",
        "            value = -self._move_to_leaf().evaluate()\n",
        "            self.w += value\n",
        "            self.n += 1\n",
        "            return value\n",
        "\n",
        "    def _move_to_leaf(self):\n",
        "        scores = np.array([i.n for i in self.child_nodes])\n",
        "        t = np.sum(scores)\n",
        "        pucb_values = np.array([],dtype=float)\n",
        "        for child_node in self.child_nodes:\n",
        "            puct = self._puct_value(child_node, t)\n",
        "            pucb_values = np.append(pucb_values, np.array([puct]))        \n",
        "        return self.child_nodes[np.argmax(pucb_values)]\n",
        "\n",
        "    def _puct_value(self, c, t):\n",
        "        return (-c.w / c.n if c.n else 0.0) + self.c_puct * c.p * sqrt(t) / (1 + c.n)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xBTIDsaKmft"
      },
      "source": [
        "@dataclass\n",
        "class Trajectory:\n",
        "    piece : np.ndarray = np.empty((0, 2, 9), float)\n",
        "    policy : np.ndarray = np.empty((0,9), float)\n",
        "    value : np.ndarray = np.array([], int)\n",
        "    code : np.ndarray = np.array([])\n",
        "\n",
        "    def reset_trajectory(self):\n",
        "        self.piece = np.empty((0, 2, 9), float)\n",
        "        self.policy = np.empty((0,9), float)\n",
        "        self.value = np.array([], int)\n",
        "        self.code = np.array([])\n",
        "\n",
        "    def set_trajectory(self, pieces, enemy_pieces, policy, value, code):\n",
        "        cc = np.concatenate([pieces, enemy_pieces])\n",
        "        cc = np.reshape(cc, [1, 2, 9])\n",
        "        self.piece = np.append(self.piece, cc, axis=0)\n",
        "        policy = np.reshape(policy, [1, 9])\n",
        "        self.policy = np.append(self.policy, policy, axis=0)\n",
        "        self.value = np.append(self.value, np.array(value))\n",
        "        self.code = np.append(self.code, np.array(code))\n",
        "\n",
        "    def get_trajectory(self):\n",
        "        return (self.piece, self.policy, self.value)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJBJqpx-nYHB"
      },
      "source": [
        "class Train:\n",
        "    def __init__(self, brain):\n",
        "        self.episodes_times = 30\n",
        "        self.brain = brain\n",
        "\n",
        "        self._train()\n",
        "\n",
        "    def _train(self):\n",
        "        trajectory = Trajectory()\n",
        "        progbar = Progbar(self.episodes_times)\n",
        "\n",
        "        for i in range(self.episodes_times):\n",
        "            self._play(self.brain, trajectory)\n",
        "\n",
        "            if (i % 5 == 0) and (i > 0):\n",
        "                self.brain.train(trajectory)\n",
        "                trajectory.reset_trajectory()\n",
        "            \n",
        "            progbar.add(1)\n",
        "\n",
        "        self.brain.save()\n",
        "\n",
        "    def _play(self, brain, trajectory):\n",
        "        state = State()\n",
        "        mcts = MonteCarloTreeSearch(brain)\n",
        "        code = self._make_random_code()\n",
        "\n",
        "        while True:\n",
        "            if state.terminal():\n",
        "                break\n",
        "\n",
        "            scores = mcts.search(state)\n",
        "            policies = np.zeros(9)\n",
        "            for action, score in zip(state.possible_actions(), scores):\n",
        "                policies[action] = score\n",
        "            trajectory.set_trajectory(state.pieces, state.enemy_pieces, policies, 0, code)\n",
        "\n",
        "            action = np.random.choice(state.possible_actions(), p = scores)\n",
        "            state = state.step(action)\n",
        "\n",
        "            value = -1 if state.first_attack() else 1\n",
        "            for i in range(len(trajectory.value)):\n",
        "                if code == trajectory.code[i] and state.judgment_lose():\n",
        "                    trajectory.value[i] = value\n",
        "                reward = -value\n",
        "\n",
        "    def _make_random_code(self, n=10):\n",
        "        return ''.join(random.choices(string.ascii_letters + string.digits, k=n))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKdTDwPV1tfL",
        "outputId": "0a2e95a9-9b79-4dc5-fdc5-6f20b23b3bf7"
      },
      "source": [
        "Train(Brain())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30/30 [==============================] - 279s 9s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Train at 0x7f5d015fc790>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}